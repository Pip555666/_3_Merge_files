이 코드는 공포탐욕지수(Fear & Greed Index) 데이터를 처리하고, 이를 분석하여 MySQL 데이터베이스에 저장하는 과정과, 시각화된 차트를 생성하는 작업을 자동화한 것입니다. 주요 목표는 각 기업의 공포탐욕지수 변화를 시간대별, 월별로 분석하고 이를 기반으로 여러 데이터를 시각화 및 클러스터링하여, 최종적으로 MySQL에 데이터를 저장하는 것입니다.

주요 기능 설명
환경 설정 및 Spark 세션 생성

setup() 함수는 한글 폰트를 설정하고, 차트를 저장할 디렉토리를 생성하며, Spark 세션을 초기화합니다.

생성된 Spark 세션은 분산 데이터 처리 환경을 설정하여, 이후 데이터를 효율적으로 처리할 수 있게 도와줍니다.

데이터 로드 및 전처리

load_and_preprocess() 함수는 CSV 파일을 Spark DataFrame으로 불러옵니다. 각 데이터는 time 컬럼을 바탕으로 날짜, 시간, 연도 등을 추출하여 공포탐욕지수를 계산합니다.

파일 경로는 스크립트가 실행되는 디렉토리 기준으로 설정되어 있어서, 코드 실행 환경에 따라 자동으로 파일을 찾습니다.

공포탐욕지수 계산

calculate_fear_greed() 함수는 시간대별 평균 공포탐욕지수와 월별 평균 공포탐욕지수를 계산합니다.

계산된 데이터는 Pandas DataFrame 형식으로 변환되어 CSV 파일로 저장됩니다.

공포탐욕지수 변화율 계산

calculate_change_rate() 함수는 공포탐욕지수의 시간대별 변화율을 계산합니다.

이전 값과 비교하여 변화율을 계산하며, 결과는 역시 CSV로 저장됩니다.

시각화

save_plots() 함수는 계산된 공포탐욕지수의 월간 변화율과 시간대별 변화를 그래프로 시각화하여 이미지 파일로 저장합니다.

공포탐욕지수의 변화율 그래프는 시간대별로, 월간 평균 그래프는 월별로 출력됩니다.

MySQL 데이터베이스 저장

insert_to_mysql() 함수는 MySQL 데이터베이스에 데이터 삽입을 담당합니다.

각 기업별로 계산된 시간대별, 월별 공포탐욕지수와 변화율 데이터를 MySQL 테이블에 삽입합니다.

이미 존재하는 데이터가 있을 경우 ON DUPLICATE KEY UPDATE 구문을 사용하여 데이터를 갱신합니다.

전체 실행 흐름

__main__ 블록에서 MySQL 데이터베이스에 연결하고, 여러 기업에 대해 데이터를 처리합니다.

각 기업의 데이터 파일을 읽고 전처리한 후, 공포탐욕지수를 계산하고 변화율을 분석한 뒤, 그래프를 생성하고 MySQL에 데이터를 저장합니다.

데이터베이스 테이블 구조
hourly_feargreed_bert

company, year, hour, 평균_공포탐욕지수

monthly_feargreed_bert

company, month, 평균_공포탐욕지수

feargreed_change_rate

company, year, hour, 변화율

코드 실행 흐름
환경 설정

차트를 저장할 디렉토리와 Spark 세션을 설정합니다.

데이터 로드 및 전처리

각 기업의 CSV 파일을 읽어오고 필요한 컬럼을 생성하여 공포탐욕지수를 계산합니다.

데이터 분석

시간대별 공포탐욕지수의 평균과 변화율을 계산합니다.

월별 평균 공포탐욕지수도 계산하여 CSV 파일로 저장합니다.

시각화

공포탐욕지수의 시간대별 변화율과 월별 평균을 그래프 형식으로 저장합니다.

MySQL 데이터베이스 저장

각 분석 결과를 MySQL 테이블에 삽입하거나 갱신합니다.

주의 사항
데이터 부족: cluster_analysis()와 같은 클러스터링 분석이나 이동 평균 분석 부분은 주석 처리되어 있으며, 필요시 활성화할 수 있습니다.

MySQL 연결: 데이터베이스에 연결할 때, db_config에서 사용자 이름과 비밀번호를 확인해야 합니다.

예외 처리: 각 단계에서 예외 처리를 통해 오류 발생 시 롤백하며, 문제를 해결할 수 있도록 로그를 남깁니다.

실행 결과
CSV 파일로 저장된 각 기업별 공포탐욕지수의 시간대별, 월별 평균 데이터

생성된 시각화 이미지 파일

MySQL 데이터베이스에 저장된 공포탐욕지수 관련 데이터

이 코드가 실행되면, 각 기업의 공포탐욕지수와 관련된 다양한 분석 결과를 MySQL에 저장하고, 시각적으로 확인할 수 있습니다.



_5_aggregating/process_sentiment_spark.py


이 코드는 여러 기업에 대한 감정 분석 데이터를 처리하고, 날짜별로 공포, 중립, 탐욕 감정 비율의 평균을 계산하여 CSV 파일로 저장하는 작업을 수행합니다. 코드의 주요 단계는 다음과 같습니다:

1. Spark 세션 설정
SparkSession.builder.appName("SentimentAggregation").getOrCreate()를 사용하여 Spark 세션을 설정합니다. 이 세션은 데이터를 분산 처리하고 분석할 수 있게 해줍니다.

2. 기업 리스트 정의
companies 리스트에는 분석할 기업들이 정의되어 있습니다. 현재는 삼성, 애플, 엔비디아, SK하이닉스가 포함되어 있으며, 필요에 따라 기업을 추가할 수 있습니다.

3. 파일 경로 설정
스크립트가 위치한 디렉토리의 경로를 기준으로 원본 데이터(raw_data_path)와 처리된 데이터를 저장할 경로(processed_data_path)를 설정합니다. 이는 상대 경로로 설정하여 다른 환경에서도 유연하게 사용할 수 있습니다.

4. 기업별 데이터 처리
for company in companies:를 사용하여 각 기업에 대해 데이터를 처리합니다.

기업마다 관련된 CSV 파일을 Spark로 읽어옵니다. 파일 경로는 file:///{raw_data_path}{company}_predict_bert.csv 형식으로 지정됩니다.

해당 파일이 존재하는지 체크하고, 없다면 경고 메시지를 출력하고 해당 기업은 건너뜁니다.

5. CSV 파일 로드 및 날짜 변환
데이터를 Spark DataFrame으로 로드하고, time 컬럼을 date 형식으로 변환하여 새로운 date 컬럼을 생성합니다. 이는 날짜별로 데이터를 집계할 때 필요합니다.

6. 날짜별 감정 비율 평균 계산
각 날짜별로 prob_fear, prob_neutral, prob_greed 감정 비율의 평균을 계산하여 df_daily라는 DataFrame을 생성합니다.

groupBy("date").agg(...)를 사용하여 날짜별 평균을 계산합니다.

7. 결과 출력 및 확인
계산된 날짜별 평균 감정 비율 데이터를 상위 5개 항목을 df_daily.show(5)로 출력하여 확인합니다.

8. Pandas로 변환 후 CSV 저장
처리된 데이터를 Pandas DataFrame으로 변환한 후, data/processed/ 폴더에 CSV 파일로 저장합니다.

파일 경로는 {company}_daily_sentiment.csv 형식으로 지정되며, utf-8-sig 인코딩 방식으로 저장됩니다.

9. 결과 출력
각 기업의 데이터가 처리된 후 저장 완료 메시지를 출력합니다.

10. Spark 세션 종료
모든 작업이 끝나면 spark.stop()을 호출하여 Spark 세션을 종료합니다.

요약
이 코드는 여러 기업에 대해 감정 분석 데이터를 처리하고, 날짜별로 감정 비율 평균을 계산하여 CSV 파일로 저장하는 자동화된 작업을 수행합니다. 주요 작업은:

데이터 파일 로드

날짜 컬럼 추가 및 감정 비율 평균 계산

결과를 CSV로 저장

이 과정은 기업별로 반복되며, 데이터가 성공적으로 처리되고 저장된 후, 최종적으로 모든 기업의 감정 분석이 완료되었음을 알립니다.



