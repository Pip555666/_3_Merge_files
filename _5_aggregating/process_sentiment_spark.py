from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, avg
import os

# âœ… Spark ì„¸ì…˜ ì„¤ì •
spark = SparkSession.builder.appName("SentimentAggregation").getOrCreate()

# âœ… íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ì •ì˜
companies = ["samsung", "apple", "nvidia", "skhynix"]  # í•„ìš”í•œ ê¸°ì—… ì¶”ê°€ ê°€ëŠ¥

# âœ… ì›ë³¸ ë°ì´í„° & ì €ì¥ í´ë” ê²½ë¡œ ì„¤ì •
# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))  # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜
except NameError:  # Jupyter Notebook ë“±ì—ì„œ __file__ì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°
    script_dir = os.getcwd()

# ë°ì´í„° ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì •)
raw_data_path = os.path.join(script_dir, "../_0_data/_3_predict/")
# ì¶œë ¥ ê²½ë¡œ (ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€)
processed_data_path = os.path.join(script_dir, "data/processed/")

# ë””ë²„ê¹…ìš© ì¶œë ¥
print(f"Raw data path: {raw_data_path}")
print(f"Processed data path: {processed_data_path}")

# âœ… ëª¨ë“  ê¸°ì—… ë°ì´í„° ì²˜ë¦¬
for company in companies:
    file_path = f"file:///{raw_data_path}{company}_predict_bert.csv"  # Sparkì—ì„œ ë¡œì»¬ íŒŒì¼ ì½ê¸° í˜•ì‹ ì ìš©

    print(f"ğŸ” Checking file: {file_path}")

    local_file_path = os.path.join(raw_data_path, f"{company}_predict_bert.csv")  # ë¡œì»¬ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ìš©
    if not os.path.exists(local_file_path):
        print(f"âš ï¸ {company} ë°ì´í„° íŒŒì¼ ì—†ìŒ: {local_file_path}")
        continue

    # ğŸ“Œ CSV íŒŒì¼ ë¡œë“œ
    df = spark.read.option("header", True).option("inferSchema", True).csv(file_path)

    # ğŸ“Œ ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
    df = df.withColumn("date", to_date(col("time")))  # "time" ì»¬ëŸ¼ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ

    # ğŸ“Œ ë‚ ì§œë³„ ê°ì • ë¹„ìœ¨ í‰ê·  ê³„ì‚°
    df_daily = df.groupBy("date").agg(
        avg("prob_fear").alias("fear_ratio"),
        avg("prob_neutral").alias("neutral_ratio"),
        avg("prob_greed").alias("greed_ratio")
    )

    # ğŸ“Œ ê²°ê³¼ í™•ì¸ (ìƒìœ„ 5ê°œ ë°ì´í„°)
    print(f"âœ… {company} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
    df_daily.show(5)

    # ğŸ“Œ Pandas ë³€í™˜ í›„ CSV ì €ì¥
    os.makedirs(processed_data_path, exist_ok=True)  # í´ë” ì—†ìœ¼ë©´ ìƒì„±
    output_file = os.path.join(processed_data_path, f"{company}_daily_sentiment.csv")
    df_daily.toPandas().to_csv(output_file, index=False, encoding="utf-8-sig")

    print(f"ğŸ’¾ {company} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")

print("ğŸ¯ ëª¨ë“  ê¸°ì—… ê°ì • ë¶„ì„ ì™„ë£Œ!")

# Spark ì„¸ì…˜ ì¢…ë£Œ
spark.stop()